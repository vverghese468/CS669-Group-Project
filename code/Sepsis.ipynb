{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# COLAB_SETUP.py\n",
        "#\n",
        "# *** RUN THIS CELL FIRST IN YOUR COLAB NOTEBOOK ***\n",
        "#\n",
        "# This cell does 4 things:\n",
        "#   1. Mounts your Google Drive\n",
        "#   2. Copies all pipeline .py files from Drive → /content/sepsis_rl_pipeline/\n",
        "#   3. Adds /content/sepsis_rl_pipeline to sys.path\n",
        "#   4. Verifies everything is in place\n",
        "#\n",
        "# HOW TO USE:\n",
        "#   - Upload all pipeline .py files to your Google Drive at:\n",
        "#       My Drive/sepsis_rl_pipeline/\n",
        "#   - Then run this cell once at the start of every Colab session\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# ── Step 1: Mount Google Drive ────────────────────────────────\n",
        "print(\"Step 1: Mounting Google Drive...\")\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "print(\"  ✓ Drive mounted\\n\")\n",
        "\n",
        "# ── Step 2: Define paths ──────────────────────────────────────\n",
        "# Where your pipeline files live ON DRIVE\n",
        "DRIVE_PIPELINE_DIR = \"/content/drive/MyDrive/sepsis_rl_pipeline\"\n",
        "\n",
        "# Where Colab will run them from (local, fast, in-memory)\n",
        "LOCAL_PIPELINE_DIR = \"/content/sepsis_rl_pipeline\"\n",
        "\n",
        "# ── Step 3: Copy files from Drive → /content/ ────────────────\n",
        "print(\"Step 2: Copying pipeline files to /content/sepsis_rl_pipeline/ ...\")\n",
        "os.makedirs(LOCAL_PIPELINE_DIR, exist_ok=True)\n",
        "\n",
        "# Check Drive folder exists\n",
        "if not os.path.exists(DRIVE_PIPELINE_DIR):\n",
        "    raise FileNotFoundError(\n",
        "        f\"\\nCould not find pipeline files on Drive at:\\n\"\n",
        "        f\"  {DRIVE_PIPELINE_DIR}\\n\\n\"\n",
        "        \"Please upload all pipeline .py files to:\\n\"\n",
        "        \"  My Drive/sepsis_rl_pipeline/\\n\"\n",
        "        \"then re-run this cell.\"\n",
        "    )\n",
        "\n",
        "# Copy every .py file\n",
        "copied = []\n",
        "for fname in os.listdir(DRIVE_PIPELINE_DIR):\n",
        "    if fname.endswith(\".py\"):\n",
        "        src = os.path.join(DRIVE_PIPELINE_DIR, fname)\n",
        "        dst = os.path.join(LOCAL_PIPELINE_DIR, fname)\n",
        "        shutil.copy2(src, dst)\n",
        "        copied.append(fname)\n",
        "        print(f\"  ✓ {fname}\")\n",
        "\n",
        "if not copied:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No .py files found in {DRIVE_PIPELINE_DIR}.\\n\"\n",
        "        \"Make sure you uploaded the pipeline files to Drive first.\"\n",
        "    )\n",
        "\n",
        "# ── Step 4: Add to sys.path ───────────────────────────────────\n",
        "print(\"\\nStep 3: Adding to sys.path...\")\n",
        "if LOCAL_PIPELINE_DIR not in sys.path:\n",
        "    sys.path.insert(0, LOCAL_PIPELINE_DIR)\n",
        "os.chdir(LOCAL_PIPELINE_DIR)\n",
        "print(f\"  ✓ sys.path updated\")\n",
        "print(f\"  ✓ Working directory set to {LOCAL_PIPELINE_DIR}\")\n",
        "\n",
        "# ── Step 5: Verify config is importable ──────────────────────\n",
        "print(\"\\nStep 4: Verifying setup...\")\n",
        "try:\n",
        "    import config\n",
        "    print(f\"  ✓ config.py loaded  (GCP_PROJECT_ID = '{config.GCP_PROJECT_ID}')\")\n",
        "except ImportError as e:\n",
        "    print(f\"  ✗ Could not import config.py: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"✓ Setup complete — you can now run the pipeline steps.\")\n",
        "print(\"=\"*55)\n",
        "print(\"\"\"\n",
        "Next steps (run each in its own cell):\n",
        "\n",
        "  exec(open(\"00_auth_drive.py\").read())   # Mount Drive + create folders\n",
        "  exec(open(\"01_auth_gcp.py\").read())     # Authenticate to BigQuery\n",
        "  exec(open(\"02_extract_data.py\").read()) # Extract data from BigQuery\n",
        "  exec(open(\"03_process_data.py\").read()) # Process features + rewards\n",
        "  exec(open(\"04_model.py\").read())        # Train RL model\n",
        "  exec(open(\"05_save_outputs.py\").read()) # Save plots + report\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGIOQ2xHBkdD",
        "outputId": "f1f578a5-11d3-47df-86e6-e2f8c2e1c037"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "  ✓ Drive mounted\n",
            "\n",
            "Step 2: Copying pipeline files to /content/sepsis_rl_pipeline/ ...\n",
            "  ✓ config.py\n",
            "\n",
            "Step 3: Adding to sys.path...\n",
            "  ✓ sys.path updated\n",
            "  ✓ Working directory set to /content/sepsis_rl_pipeline\n",
            "\n",
            "Step 4: Verifying setup...\n",
            "  ✓ config.py loaded  (GCP_PROJECT_ID = 'silken-physics-467815-g5')\n",
            "\n",
            "=======================================================\n",
            "✓ Setup complete — you can now run the pipeline steps.\n",
            "=======================================================\n",
            "\n",
            "Next steps (run each in its own cell):\n",
            "\n",
            "  exec(open(\"00_auth_drive.py\").read())   # Mount Drive + create folders\n",
            "  exec(open(\"01_auth_gcp.py\").read())     # Authenticate to BigQuery\n",
            "  exec(open(\"02_extract_data.py\").read()) # Extract data from BigQuery\n",
            "  exec(open(\"03_process_data.py\").read()) # Process features + rewards\n",
            "  exec(open(\"04_model.py\").read())        # Train RL model\n",
            "  exec(open(\"05_save_outputs.py\").read()) # Save plots + report\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dVNdhhfEw_0q"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# config.py  —  Central configuration for the sepsis RL pipeline\n",
        "#\n",
        "# Running in Google Colab — no credential files needed.\n",
        "# Authentication is handled by google.colab.auth (browser popup).\n",
        "#\n",
        "# *** The only value you must change before running: ***\n",
        "#     GCP_PROJECT_ID  (line 14)\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# GCP / BigQuery\n",
        "# ----------------------------------------------------------------\n",
        "GCP_PROJECT_ID = \"silken-physics-467815-g5\"   # ← CHANGE THIS\n",
        "MIMIC_DATASET  = \"physionet-data.mimiciii_clinical\"\n",
        "\n",
        "# No key files or OAuth JSON needed — Colab authenticates as\n",
        "# your logged-in Google account via a one-time browser popup.\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Drive folder layout\n",
        "# All paths are relative to your Drive root (\"My Drive\")\n",
        "# ----------------------------------------------------------------\n",
        "DRIVE_BASE_FOLDER   = \"sepsis_rl\"          # top-level folder\n",
        "DRIVE_RAW_FOLDER    = \"raw\"                # raw BigQuery extracts\n",
        "DRIVE_PROC_FOLDER   = \"processed\"          # windowed / feature-engineered data\n",
        "DRIVE_OUTPUT_FOLDER = \"outputs\"            # model artefacts + metrics\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Pipeline parameters\n",
        "# ----------------------------------------------------------------\n",
        "WINDOW_HOURS      = 4          # length of each time window\n",
        "MAX_WINDOWS       = 20         # max windows per stay  (= 80 hours)\n",
        "N_FLUID_BINS      = 5          # discrete fluid action levels  (0–4)\n",
        "N_VASO_BINS       = 5          # discrete vasopressor levels   (0–4)\n",
        "TERMINAL_REWARD   = 15.0       # ±15 reward at end of episode\n",
        "SOFA_PENALTY      = 0.5        # weight on SOFA-change shaping reward\n",
        "MORTALITY_HORIZON = 90         # days for primary outcome (90-day mortality)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# File names written to Drive\n",
        "# ----------------------------------------------------------------\n",
        "RAW_COHORT_FILE   = \"cohort.parquet\"\n",
        "RAW_FEATURES_FILE = \"features.parquet\"\n",
        "RAW_ACTIONS_FILE  = \"actions_raw.parquet\"\n",
        "RAW_MORTALITY_FILE= \"mortality.parquet\"\n",
        "\n",
        "PROC_STATES_FILE  = \"states.parquet\"\n",
        "PROC_ACTIONS_FILE = \"actions_binned.parquet\"\n",
        "PROC_DATASET_FILE = \"sepsis_rl_dataset.parquet\"\n",
        "\n",
        "OUTPUT_MODEL_FILE   = \"model.pt\"\n",
        "OUTPUT_METRICS_FILE = \"metrics.json\"\n",
        "OUTPUT_PLOTS_DIR    = \"plots\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 00_auth_drive.py  —  Google Drive auth for Google Colab\n",
        "#\n",
        "# Uses Colab's built-in drive.mount() — no credential files,\n",
        "# no OAuth2 JSON, no token files. Just a one-click browser prompt.\n",
        "#\n",
        "# Drive is mounted as a local filesystem at /content/drive/\n",
        "# so all file operations use standard Python file I/O —\n",
        "# no upload/download API calls needed.\n",
        "# ============================================================\n",
        "\n",
        "import sys, os\n",
        "\n",
        "# ── Colab path bootstrap ─────────────────────────────────────\n",
        "# Tells Python where to find config.py and other modules.\n",
        "# Update PIPELINE_DIR if your files are in a different location.\n",
        "PIPELINE_DIR = \"/content/sepsis_rl_pipeline\"\n",
        "if PIPELINE_DIR not in sys.path:\n",
        "    sys.path.insert(0, PIPELINE_DIR)\n",
        "os.chdir(PIPELINE_DIR)  # also set working directory\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Where Colab mounts your Drive\n",
        "DRIVE_MOUNT_POINT = \"/content/drive/MyDrive\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MOUNT DRIVE\n",
        "# ============================================================\n",
        "\n",
        "def mount_drive():\n",
        "    \"\"\"\n",
        "    Mount Google Drive into the Colab filesystem.\n",
        "    Shows a one-time browser prompt asking you to allow access.\n",
        "    After that, your Drive is accessible like a normal folder.\n",
        "    \"\"\"\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    print(f\"  Google Drive mounted at /content/drive\")\n",
        "    print(f\"  Your files are at: {DRIVE_MOUNT_POINT}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FOLDER HELPERS\n",
        "# (Just local filesystem ops — Drive looks like a normal folder)\n",
        "# ============================================================\n",
        "\n",
        "def get_folder_path(relative_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Build an absolute path inside your Drive.\n",
        "    e.g. get_folder_path(\"sepsis_rl/raw\")\n",
        "         → \"/content/drive/MyDrive/sepsis_rl/raw\"\n",
        "    \"\"\"\n",
        "    return os.path.join(DRIVE_MOUNT_POINT, relative_path)\n",
        "\n",
        "\n",
        "def setup_drive_folders() -> dict:\n",
        "    \"\"\"\n",
        "    Create the pipeline folder structure inside Drive if it doesn't exist.\n",
        "    Returns a dict of { name: absolute_path } for each folder.\n",
        "    \"\"\"\n",
        "    import config\n",
        "\n",
        "    folders = {\n",
        "        \"base\":      get_folder_path(config.DRIVE_BASE_FOLDER),\n",
        "        \"raw\":       get_folder_path(f\"{config.DRIVE_BASE_FOLDER}/{config.DRIVE_RAW_FOLDER}\"),\n",
        "        \"processed\": get_folder_path(f\"{config.DRIVE_BASE_FOLDER}/{config.DRIVE_PROC_FOLDER}\"),\n",
        "        \"outputs\":   get_folder_path(f\"{config.DRIVE_BASE_FOLDER}/{config.DRIVE_OUTPUT_FOLDER}\"),\n",
        "    }\n",
        "\n",
        "    for name, path in folders.items():\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        print(f\"  Ready: {path}\")\n",
        "\n",
        "    return folders\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FILE HELPERS\n",
        "# (Simple wrappers so other scripts stay consistent)\n",
        "# ============================================================\n",
        "\n",
        "def save_parquet(df, folder_path: str, filename: str):\n",
        "    \"\"\"Save a DataFrame as parquet directly into a Drive folder.\"\"\"\n",
        "    path = os.path.join(folder_path, filename)\n",
        "    df.to_parquet(path, index=False)\n",
        "    print(f\"  Saved → {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def load_parquet(folder_path: str, filename: str):\n",
        "    \"\"\"Load a parquet file directly from a Drive folder.\"\"\"\n",
        "    import pandas as pd\n",
        "    path = os.path.join(folder_path, filename)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"File not found: {path}\\n\"\n",
        "            \"Make sure the previous pipeline step has been run.\"\n",
        "        )\n",
        "    df = pd.read_parquet(path)\n",
        "    print(f\"  Loaded {len(df):,} rows ← {path}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_json(data: dict, folder_path: str, filename: str):\n",
        "    \"\"\"Save a dict as JSON directly into a Drive folder.\"\"\"\n",
        "    import json\n",
        "    path = os.path.join(folder_path, filename)\n",
        "    with open(path, \"w\") as fh:\n",
        "        json.dump(data, fh, indent=2)\n",
        "    print(f\"  Saved → {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def load_json(folder_path: str, filename: str) -> dict:\n",
        "    \"\"\"Load a JSON file directly from a Drive folder.\"\"\"\n",
        "    import json\n",
        "    path = os.path.join(folder_path, filename)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    with open(path) as fh:\n",
        "        return json.load(fh)\n",
        "\n",
        "\n",
        "def save_file(src_path: str, folder_path: str):\n",
        "    \"\"\"Copy any file (e.g. .pt, .png) into a Drive folder.\"\"\"\n",
        "    import shutil\n",
        "    filename = os.path.basename(src_path)\n",
        "    dst_path = os.path.join(folder_path, filename)\n",
        "    shutil.copy2(src_path, dst_path)\n",
        "    print(f\"  Saved → {dst_path}\")\n",
        "    return dst_path\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 55)\n",
        "    print(\"STEP 0: Google Drive Mount & Folder Setup\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    mount_drive()\n",
        "    folders = setup_drive_folders()\n",
        "\n",
        "    print(\"\\nDrive folder paths:\")\n",
        "    for k, v in folders.items():\n",
        "        print(f\"  {k:12s} →  {v}\")\n",
        "\n",
        "    print(\"\\nDrive setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UWlIlVUzTwC",
        "outputId": "c796b827-438d-467a-8088-90a981488a91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "STEP 0: Google Drive Mount & Folder Setup\n",
            "=======================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "  Google Drive mounted at /content/drive\n",
            "  Your files are at: /content/drive/MyDrive\n",
            "  Ready: /content/drive/MyDrive/sepsis_rl\n",
            "  Ready: /content/drive/MyDrive/sepsis_rl/raw\n",
            "  Ready: /content/drive/MyDrive/sepsis_rl/processed\n",
            "  Ready: /content/drive/MyDrive/sepsis_rl/outputs\n",
            "\n",
            "Drive folder paths:\n",
            "  base         →  /content/drive/MyDrive/sepsis_rl\n",
            "  raw          →  /content/drive/MyDrive/sepsis_rl/raw\n",
            "  processed    →  /content/drive/MyDrive/sepsis_rl/processed\n",
            "  outputs      →  /content/drive/MyDrive/sepsis_rl/outputs\n",
            "\n",
            "Drive setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 01_auth_gcp.py  —  GCP / BigQuery auth for Google Colab\n",
        "#\n",
        "# Uses google.colab.auth.authenticate_user() — no service account\n",
        "# key file, no JSON credentials on disk.\n",
        "#\n",
        "# Colab authenticates as YOUR Google account (the one logged\n",
        "# into the notebook), so you just click \"Allow\" in a popup\n",
        "# once per session. That's it.\n",
        "#\n",
        "# Your account needs these IAM roles on the GCP project:\n",
        "#   - BigQuery Data Viewer\n",
        "#   - BigQuery Job User\n",
        "#\n",
        "# Public helpers used by other steps:\n",
        "#   get_bq_client()            →  google.cloud.bigquery.Client\n",
        "#   run_query(client, sql, params)  →  pd.DataFrame\n",
        "# ============================================================\n",
        "\n",
        "import sys, os\n",
        "\n",
        "# ── Colab path bootstrap ─────────────────────────────────────\n",
        "# The pipeline files must be in /content/sepsis_rl_pipeline/\n",
        "# Run the SETUP CELL in the Colab notebook first — it copies\n",
        "# everything from Drive into this location automatically.\n",
        "PIPELINE_DIR = \"/content/sepsis_rl_pipeline\"\n",
        "os.makedirs(PIPELINE_DIR, exist_ok=True)   # safe to call if already exists\n",
        "if PIPELINE_DIR not in sys.path:\n",
        "    sys.path.insert(0, PIPELINE_DIR)\n",
        "os.chdir(PIPELINE_DIR)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "from google.colab  import auth\n",
        "from google.cloud  import bigquery\n",
        "import google.auth\n",
        "\n",
        "import config\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Authentication\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def authenticate_gcp():\n",
        "    \"\"\"\n",
        "    Trigger Colab's built-in GCP authentication popup.\n",
        "    Safe to call multiple times — only prompts once per session.\n",
        "    \"\"\"\n",
        "    auth.authenticate_user()\n",
        "    print(\"  GCP authentication complete (logged in as your Google account).\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Client factory\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def get_bq_client() -> bigquery.Client:\n",
        "    \"\"\"\n",
        "    Return an authenticated BigQuery client using Colab credentials.\n",
        "    Call authenticate_gcp() before this if not already done.\n",
        "    \"\"\"\n",
        "    credentials, _ = google.auth.default(\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "    )\n",
        "    client = bigquery.Client(\n",
        "        project=config.GCP_PROJECT_ID,\n",
        "        credentials=credentials,\n",
        "    )\n",
        "    print(f\"  BigQuery client ready  (project: {config.GCP_PROJECT_ID})\")\n",
        "    return client\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Query helper\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def run_query(\n",
        "    client: bigquery.Client,\n",
        "    sql: str,\n",
        "    params: list | None = None,\n",
        ") -> \"pd.DataFrame\":\n",
        "    \"\"\"\n",
        "    Execute a BigQuery SQL string and return results as a DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    client : bigquery.Client\n",
        "    sql    : str   — Standard SQL (may contain @param placeholders)\n",
        "    params : list  — Optional list of bigquery.*QueryParameter objects\n",
        "    \"\"\"\n",
        "    job_config = bigquery.QueryJobConfig(query_parameters=params or [])\n",
        "    job        = client.query(sql, job_config=job_config)\n",
        "    return job.result().to_dataframe()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Validation\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "def validate_bq_access(client: bigquery.Client):\n",
        "    \"\"\"\n",
        "    Run a cheap COUNT query to confirm MIMIC-III access.\n",
        "    \"\"\"\n",
        "    test_sql = f\"\"\"\n",
        "        SELECT COUNT(*) AS n_stays\n",
        "        FROM `{config.MIMIC_DATASET}.icustays`\n",
        "        LIMIT 1\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = run_query(client, test_sql)\n",
        "        n  = df[\"n_stays\"].iloc[0]\n",
        "        print(f\"  MIMIC-III access confirmed — {n:,} ICU stays found.\")\n",
        "    except Exception as exc:\n",
        "        print(\n",
        "            f\"\\n  ERROR: Could not query MIMIC-III.\\n\"\n",
        "            f\"  Reason: {exc}\\n\\n\"\n",
        "            \"  Checklist:\\n\"\n",
        "            \"    1. GCP_PROJECT_ID is set correctly in config.py\\n\"\n",
        "            \"    2. Your Google account has BigQuery Data Viewer +\\n\"\n",
        "            \"       BigQuery Job User roles on the project\\n\"\n",
        "            \"    3. Your GCP project is linked to PhysioNet:\\n\"\n",
        "            \"       https://physionet.org/settings/credentialing/\\n\"\n",
        "            \"    4. BigQuery API is enabled in your GCP project\\n\"\n",
        "        )\n",
        "        raise\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Entry point\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 55)\n",
        "    print(\"STEP 1: GCP / BigQuery Authentication (Colab)\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    authenticate_gcp()\n",
        "    client = get_bq_client()\n",
        "    validate_bq_access(client)\n",
        "\n",
        "    print(\"\\nReady to query BigQuery.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mJKarF30lyN",
        "outputId": "2864bd80-c300-4a73-ccfb-279235f10f92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "STEP 1: GCP / BigQuery Authentication (Colab)\n",
            "=======================================================\n",
            "  GCP authentication complete (logged in as your Google account).\n",
            "  BigQuery client ready  (project: silken-physics-467815-g5)\n",
            "  MIMIC-III access confirmed — 61,532 ICU stays found.\n",
            "\n",
            "Ready to query BigQuery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRACT DATA**"
      ],
      "metadata": {
        "id": "-lPrQRIn5-d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 02_extract_data.py  —  Extract MIMIC-III data from BigQuery\n",
        "#                         and save raw parquet files to Drive\n",
        "#\n",
        "# SELF-CONTAINED: no imports from other pipeline files needed.\n",
        "# Just paste this entire file into a Colab cell and run it.\n",
        "#\n",
        "# Outputs written to Drive → My Drive/sepsis_rl/raw/\n",
        "#   cohort.parquet\n",
        "#   features.parquet\n",
        "#   actions_raw.parquet\n",
        "#   mortality.parquet\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive, auth\n",
        "from google.cloud  import bigquery\n",
        "import google.auth\n",
        "\n",
        "# ============================================================\n",
        "# ★  CONFIG — only thing you need to change\n",
        "# ============================================================\n",
        "GCP_PROJECT_ID = \"silken-physics-467815-g5\"       # ← CHANGE THIS\n",
        "MIMIC_DATASET  = \"physionet-data.mimiciii_clinical\"\n",
        "\n",
        "DRIVE_BASE     = \"/content/drive/MyDrive/sepsis_rl\"\n",
        "RAW_DIR        = os.path.join(DRIVE_BASE, \"raw\")\n",
        "\n",
        "# ============================================================\n",
        "# AUTH — Mount Drive + Authenticate GCP\n",
        "# ============================================================\n",
        "print(\"=\"*55)\n",
        "print(\"STEP 2: Data Extraction → Google Drive\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "print(\"\\n[Auth] Mounting Drive...\")\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "print(\"[Auth] Authenticating to GCP...\")\n",
        "auth.authenticate_user()\n",
        "credentials, _ = google.auth.default(\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        ")\n",
        "client = bigquery.Client(project=GCP_PROJECT_ID, credentials=credentials)\n",
        "print(f\"  ✓ BigQuery client ready  (project: {GCP_PROJECT_ID})\")\n",
        "\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "print(f\"  ✓ Output folder ready: {RAW_DIR}\")\n",
        "\n",
        "# ============================================================\n",
        "# QUERY HELPER\n",
        "# ============================================================\n",
        "def run_query(sql, params=None):\n",
        "    job_config = bigquery.QueryJobConfig(query_parameters=params or [])\n",
        "    return client.query(sql, job_config=job_config).result().to_dataframe()\n",
        "\n",
        "# ============================================================\n",
        "# SQL DEFINITIONS\n",
        "# ============================================================\n",
        "COHORT_SQL = f\"\"\"\n",
        "WITH\n",
        "icu_adults AS (\n",
        "  SELECT ie.subject_id, ie.hadm_id, ie.icustay_id,\n",
        "    ie.intime, ie.outtime,\n",
        "    DATETIME_DIFF(ie.outtime, ie.intime, HOUR) AS los_hours,\n",
        "    DATETIME_DIFF(ie.intime, p.dob, YEAR)      AS age\n",
        "  FROM `{MIMIC_DATASET}.icustays` ie\n",
        "  JOIN `{MIMIC_DATASET}.patients` p USING (subject_id)\n",
        "  WHERE DATETIME_DIFF(ie.intime, p.dob, YEAR) >= 18\n",
        "    AND DATETIME_DIFF(ie.outtime, ie.intime, HOUR) BETWEEN 12 AND 240\n",
        "),\n",
        "abx AS (\n",
        "  SELECT DISTINCT hadm_id, startdate AS abx_date\n",
        "  FROM `{MIMIC_DATASET}.prescriptions`\n",
        "  WHERE LOWER(drug) IN (\n",
        "    'vancomycin','piperacillin','meropenem','ceftriaxone',\n",
        "    'levofloxacin','metronidazole','ciprofloxacin','ampicillin',\n",
        "    'cefepime','azithromycin','fluconazole','micafungin'\n",
        "  )\n",
        "),\n",
        "cultures AS (\n",
        "  SELECT DISTINCT hadm_id, DATE(charttime) AS cult_date\n",
        "  FROM `{MIMIC_DATASET}.microbiologyevents`\n",
        "),\n",
        "suspected_infection AS (\n",
        "  SELECT a.hadm_id FROM abx a\n",
        "  JOIN cultures c USING (hadm_id)\n",
        "  WHERE ABS(DATE_DIFF(a.abx_date, c.cult_date, DAY)) <= 1\n",
        ")\n",
        "SELECT ia.* FROM icu_adults ia\n",
        "JOIN suspected_infection si USING (hadm_id)\n",
        "ORDER BY subject_id, intime\n",
        "\"\"\"\n",
        "\n",
        "VITALS_SQL = \"\"\"\n",
        "SELECT ce.icustay_id, ce.charttime,\n",
        "  CASE\n",
        "    WHEN ce.itemid IN (211,220045)                                  THEN 'heart_rate'\n",
        "    WHEN ce.itemid IN (51,442,455,6701,220179,220050)               THEN 'sysbp'\n",
        "    WHEN ce.itemid IN (8368,8440,8441,8555,220180,220051)           THEN 'diasbp'\n",
        "    WHEN ce.itemid IN (456,52,6702,443,220052,220181,225312)        THEN 'meanbp'\n",
        "    WHEN ce.itemid IN (618,615,220210,224690)                       THEN 'resp_rate'\n",
        "    WHEN ce.itemid IN (223761,678)                                  THEN 'temp_f'\n",
        "    WHEN ce.itemid IN (223762,676)                                  THEN 'temp_c'\n",
        "    WHEN ce.itemid IN (646,220277)                                  THEN 'spo2'\n",
        "    WHEN ce.itemid IN (807,811,1529,3745,3744,225664,220621,226537) THEN 'glucose'\n",
        "    WHEN ce.itemid = 226730                                         THEN 'weight_kg'\n",
        "  END AS feature,\n",
        "  ce.valuenum\n",
        "FROM `{dataset}.chartevents` ce\n",
        "WHERE ce.icustay_id IN UNNEST(@icustay_ids)\n",
        "  AND ce.error IS DISTINCT FROM 1\n",
        "  AND ce.itemid IN (\n",
        "    211,220045,51,442,455,6701,220179,220050,\n",
        "    8368,8440,8441,8555,220180,220051,\n",
        "    456,52,6702,443,220052,220181,225312,\n",
        "    618,615,220210,224690,223761,678,223762,676,\n",
        "    646,220277,807,811,1529,3745,3744,225664,220621,226537,226730\n",
        "  )\n",
        "  AND ce.valuenum IS NOT NULL AND ce.valuenum > 0\n",
        "\"\"\"\n",
        "\n",
        "LABS_SQL = \"\"\"\n",
        "SELECT le.hadm_id, le.charttime,\n",
        "  CASE\n",
        "    WHEN le.itemid = 50912 THEN 'creatinine'   WHEN le.itemid = 50902 THEN 'chloride'\n",
        "    WHEN le.itemid = 50882 THEN 'bicarbonate'  WHEN le.itemid = 50893 THEN 'calcium'\n",
        "    WHEN le.itemid = 50960 THEN 'magnesium'    WHEN le.itemid = 50983 THEN 'sodium'\n",
        "    WHEN le.itemid = 50971 THEN 'potassium'    WHEN le.itemid = 51006 THEN 'bun'\n",
        "    WHEN le.itemid = 51221 THEN 'hematocrit'   WHEN le.itemid = 51222 THEN 'hemoglobin'\n",
        "    WHEN le.itemid = 51265 THEN 'platelets'    WHEN le.itemid = 51301 THEN 'wbc'\n",
        "    WHEN le.itemid = 50813 THEN 'lactate'      WHEN le.itemid = 50820 THEN 'ph'\n",
        "    WHEN le.itemid = 50821 THEN 'pao2'         WHEN le.itemid = 50818 THEN 'paco2'\n",
        "    WHEN le.itemid = 50811 THEN 'base_excess'  WHEN le.itemid = 50861 THEN 'alt'\n",
        "    WHEN le.itemid = 50878 THEN 'ast'          WHEN le.itemid = 50863 THEN 'alp'\n",
        "    WHEN le.itemid = 50885 THEN 'bilirubin_total' WHEN le.itemid = 51275 THEN 'ptt'\n",
        "    WHEN le.itemid = 51237 THEN 'inr'          WHEN le.itemid = 50889 THEN 'crp'\n",
        "    WHEN le.itemid = 50931 THEN 'glucose_lab'  WHEN le.itemid = 51484 THEN 'bands'\n",
        "  END AS feature,\n",
        "  le.valuenum\n",
        "FROM `{dataset}.labevents` le\n",
        "WHERE le.hadm_id IN UNNEST(@hadm_ids)\n",
        "  AND le.valuenum IS NOT NULL\n",
        "  AND le.itemid IN (\n",
        "    50912,50902,50882,50893,50960,50983,50971,51006,\n",
        "    51221,51222,51265,51301,50813,50820,50821,50818,\n",
        "    50811,50861,50878,50863,50885,51275,51237,50889,50931,51484\n",
        "  )\n",
        "\"\"\"\n",
        "\n",
        "GCS_SQL = \"\"\"\n",
        "SELECT icustay_id, charttime, 'gcs' AS feature,\n",
        "       CAST(valuenum AS FLOAT64) AS valuenum\n",
        "FROM `{dataset}.chartevents`\n",
        "WHERE icustay_id IN UNNEST(@icustay_ids)\n",
        "  AND itemid IN (198, 226755, 227013)\n",
        "  AND valuenum IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "URINE_SQL = \"\"\"\n",
        "SELECT icustay_id, charttime, 'urine_output' AS feature, value AS valuenum\n",
        "FROM `{dataset}.outputevents`\n",
        "WHERE icustay_id IN UNNEST(@icustay_ids)\n",
        "  AND itemid IN (\n",
        "    40055,43175,40069,40094,40715,40473,40085,40057,40056,\n",
        "    40405,40428,40086,40096,40651,\n",
        "    226559,226560,226561,226584,226563,226564,\n",
        "    226565,226567,226557,226558,227488,227489\n",
        "  )\n",
        "  AND value > 0\n",
        "\"\"\"\n",
        "\n",
        "ACTIONS_SQL = \"\"\"\n",
        "WITH\n",
        "fluids AS (\n",
        "  SELECT icustay_id, starttime AS charttime, 'iv_fluid' AS drug_type, amount AS dose\n",
        "  FROM `{dataset}.inputevents_mv`\n",
        "  WHERE icustay_id IN UNNEST(@icustay_ids)\n",
        "    AND itemid IN (\n",
        "      225158,225943,226089,225168,225828,225823,220862,\n",
        "      220970,220864,225159,220995,225170,225825,227531,229268,227072\n",
        "    )\n",
        "    AND amount > 0 AND amountuom = 'mL'\n",
        "),\n",
        "vasopressors AS (\n",
        "  SELECT icustay_id, starttime AS charttime, 'vasopressor' AS drug_type,\n",
        "    CASE\n",
        "      WHEN itemid IN (221906,221289) THEN rate\n",
        "      WHEN itemid = 221662           THEN rate * 0.1\n",
        "      WHEN itemid = 221749           THEN rate * 10\n",
        "      WHEN itemid = 222315           THEN rate * 0.4\n",
        "    END AS dose\n",
        "  FROM `{dataset}.inputevents_mv`\n",
        "  WHERE icustay_id IN UNNEST(@icustay_ids)\n",
        "    AND itemid IN (221906,221289,221662,221749,222315)\n",
        "    AND rate > 0\n",
        ")\n",
        "SELECT * FROM fluids UNION ALL SELECT * FROM vasopressors\n",
        "\"\"\"\n",
        "\n",
        "MORTALITY_SQL = \"\"\"\n",
        "SELECT ie.icustay_id,\n",
        "  CASE WHEN p.dod IS NOT NULL\n",
        "            AND p.dod <= DATETIME_ADD(ie.outtime, INTERVAL 90 DAY)\n",
        "       THEN 1 ELSE 0 END AS died_90d,\n",
        "  CASE WHEN p.dod IS NOT NULL\n",
        "            AND p.dod <= DATETIME_ADD(ie.intime,  INTERVAL 28 DAY)\n",
        "       THEN 1 ELSE 0 END AS died_28d\n",
        "FROM `{dataset}.icustays` ie\n",
        "JOIN `{dataset}.patients` p USING (subject_id)\n",
        "WHERE ie.icustay_id IN UNNEST(@icustay_ids)\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# EXTRACT\n",
        "# ============================================================\n",
        "\n",
        "# --- Cohort ---\n",
        "print(\"\\n[1/4] Extracting sepsis cohort...\")\n",
        "cohort = run_query(COHORT_SQL)\n",
        "cohort.to_parquet(os.path.join(RAW_DIR, \"cohort.parquet\"), index=False)\n",
        "print(f\"  ✓ {len(cohort):,} ICU stays  →  cohort.parquet\")\n",
        "\n",
        "icustay_ids = cohort[\"icustay_id\"].tolist()\n",
        "hadm_ids    = cohort[\"hadm_id\"].tolist()\n",
        "icu_params  = [bigquery.ArrayQueryParameter(\"icustay_ids\", \"INT64\", icustay_ids)]\n",
        "adm_params  = [bigquery.ArrayQueryParameter(\"hadm_ids\",    \"INT64\", hadm_ids)]\n",
        "\n",
        "# --- Features ---\n",
        "print(\"\\n[2/4] Extracting clinical features...\")\n",
        "print(\"  Fetching vitals...\")\n",
        "vitals = run_query(VITALS_SQL.format(dataset=MIMIC_DATASET), icu_params)\n",
        "vitals = vitals.merge(cohort[[\"icustay_id\",\"hadm_id\"]], on=\"icustay_id\")\n",
        "\n",
        "print(\"  Fetching labs...\")\n",
        "labs = run_query(LABS_SQL.format(dataset=MIMIC_DATASET), adm_params)\n",
        "labs = labs.merge(cohort[[\"hadm_id\",\"icustay_id\"]], on=\"hadm_id\")\n",
        "\n",
        "print(\"  Fetching GCS...\")\n",
        "gcs = run_query(GCS_SQL.format(dataset=MIMIC_DATASET), icu_params)\n",
        "\n",
        "print(\"  Fetching urine output...\")\n",
        "urine = run_query(URINE_SQL.format(dataset=MIMIC_DATASET), icu_params)\n",
        "urine.rename(columns={\"value\": \"valuenum\"}, inplace=True, errors=\"ignore\")\n",
        "\n",
        "features = pd.concat([\n",
        "    vitals[[\"icustay_id\",\"charttime\",\"feature\",\"valuenum\"]],\n",
        "    labs  [[\"icustay_id\",\"charttime\",\"feature\",\"valuenum\"]],\n",
        "    gcs   [[\"icustay_id\",\"charttime\",\"feature\",\"valuenum\"]],\n",
        "    urine [[\"icustay_id\",\"charttime\",\"feature\",\"valuenum\"]],\n",
        "], ignore_index=True)\n",
        "features.dropna(subset=[\"feature\",\"valuenum\"], inplace=True)\n",
        "features.to_parquet(os.path.join(RAW_DIR, \"features.parquet\"), index=False)\n",
        "print(f\"  ✓ {len(features):,} rows | {features['feature'].nunique()} features  →  features.parquet\")\n",
        "\n",
        "# --- Actions ---\n",
        "print(\"\\n[3/4] Extracting actions (fluids + vasopressors)...\")\n",
        "actions_raw = run_query(ACTIONS_SQL.format(dataset=MIMIC_DATASET), icu_params)\n",
        "actions_raw.to_parquet(os.path.join(RAW_DIR, \"actions_raw.parquet\"), index=False)\n",
        "print(f\"  ✓ {len(actions_raw):,} rows  →  actions_raw.parquet\")\n",
        "\n",
        "# --- Mortality ---\n",
        "print(\"\\n[4/4] Extracting mortality outcomes...\")\n",
        "mortality = run_query(MORTALITY_SQL.format(dataset=MIMIC_DATASET), icu_params)\n",
        "mortality.to_parquet(os.path.join(RAW_DIR, \"mortality.parquet\"), index=False)\n",
        "print(f\"  ✓ 90-day mortality: {mortality['died_90d'].mean():.1%}  →  mortality.parquet\")\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(f\"✓ All raw data saved to: {RAW_DIR}\")\n",
        "print(\"=\"*55)\n",
        "for f in [\"cohort.parquet\",\"features.parquet\",\"actions_raw.parquet\",\"mortality.parquet\"]:\n",
        "    path = os.path.join(RAW_DIR, f)\n",
        "    size_mb = os.path.getsize(path) / 1e6 if os.path.exists(path) else 0\n",
        "    print(f\"  {f:35s}  {size_mb:.1f} MB\")\n",
        "print(\"\\nNext step → run 03_process_data.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5RCXCg-L8NE",
        "outputId": "cf567d13-0acd-4e40-a02a-d9b636967e40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "STEP 2: Data Extraction → Google Drive\n",
            "=======================================================\n",
            "\n",
            "[Auth] Mounting Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Auth] Authenticating to GCP...\n",
            "  ✓ BigQuery client ready  (project: silken-physics-467815-g5)\n",
            "  ✓ Output folder ready: /content/drive/MyDrive/sepsis_rl/raw\n",
            "\n",
            "[1/4] Extracting sepsis cohort...\n",
            "  ✓ 88,544 ICU stays  →  cohort.parquet\n",
            "\n",
            "[2/4] Extracting clinical features...\n",
            "  Fetching vitals...\n",
            "  Fetching labs...\n",
            "  Fetching GCS...\n",
            "  Fetching urine output...\n",
            "  ✓ 102,961,257 rows | 38 features  →  features.parquet\n",
            "\n",
            "[3/4] Extracting actions (fluids + vasopressors)...\n",
            "  ✓ 83,893 rows  →  actions_raw.parquet\n",
            "\n",
            "[4/4] Extracting mortality outcomes...\n",
            "  ✓ 90-day mortality: 26.9%  →  mortality.parquet\n",
            "\n",
            "=======================================================\n",
            "✓ All raw data saved to: /content/drive/MyDrive/sepsis_rl/raw\n",
            "=======================================================\n",
            "  cohort.parquet                       1.3 MB\n",
            "  features.parquet                     403.3 MB\n",
            "  actions_raw.parquet                  1.5 MB\n",
            "  mortality.parquet                    0.1 MB\n",
            "\n",
            "Next step → run 03_process_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROCESS DATA**"
      ],
      "metadata": {
        "id": "AA9gPrMo06BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 03_process_data.py  —  Feature engineering and reward assignment\n",
        "#\n",
        "# SELF-CONTAINED: no imports from other pipeline files needed.\n",
        "# Just paste this entire file into a Colab cell and run it.\n",
        "#\n",
        "# Reads from Drive  → My Drive/sepsis_rl/raw/\n",
        "# Writes to Drive   → My Drive/sepsis_rl/processed/\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================================\n",
        "# ★  CONFIG\n",
        "# ============================================================\n",
        "DRIVE_BASE    = \"/content/drive/MyDrive/sepsis_rl\"\n",
        "RAW_DIR       = os.path.join(DRIVE_BASE, \"raw\")\n",
        "PROC_DIR      = os.path.join(DRIVE_BASE, \"processed\")\n",
        "\n",
        "WINDOW_HOURS  = 4\n",
        "MAX_WINDOWS   = 20\n",
        "N_FLUID_BINS  = 5\n",
        "N_VASO_BINS   = 5\n",
        "TERMINAL_REWARD = 15.0\n",
        "SOFA_PENALTY    = 0.5\n",
        "\n",
        "# ============================================================\n",
        "# MOUNT DRIVE\n",
        "# ============================================================\n",
        "print(\"=\"*55)\n",
        "print(\"STEP 3: Data Processing\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "print(f\"  ✓ Drive mounted\")\n",
        "print(f\"  ✓ Output folder ready: {PROC_DIR}\")\n",
        "\n",
        "# ============================================================\n",
        "# LOAD RAW DATA\n",
        "# ============================================================\n",
        "print(\"\\n[Loading raw data from Drive]\")\n",
        "\n",
        "def load(filename):\n",
        "    path = os.path.join(RAW_DIR, filename)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Not found: {path}\\nRun 02_extract_data.py first.\")\n",
        "    df = pd.read_parquet(path)\n",
        "    print(f\"  ✓ {filename}  ({len(df):,} rows)\")\n",
        "    return df\n",
        "\n",
        "cohort      = load(\"cohort.parquet\")\n",
        "features    = load(\"features.parquet\")\n",
        "raw_actions = load(\"actions_raw.parquet\")\n",
        "mortality   = load(\"mortality.parquet\")\n",
        "\n",
        "cohort[\"intime\"]         = pd.to_datetime(cohort[\"intime\"])\n",
        "cohort[\"outtime\"]        = pd.to_datetime(cohort[\"outtime\"])\n",
        "features[\"charttime\"]    = pd.to_datetime(features[\"charttime\"])\n",
        "\n",
        "# ============================================================\n",
        "# A — Create 4-hour time windows\n",
        "# ============================================================\n",
        "print(\"\\n[A] Creating 4-hour time windows...\")\n",
        "rows = []\n",
        "for _, r in cohort.iterrows():\n",
        "    for w in range(MAX_WINDOWS):\n",
        "        ws = r[\"intime\"] + pd.Timedelta(hours=w * WINDOW_HOURS)\n",
        "        we = ws + pd.Timedelta(hours=WINDOW_HOURS)\n",
        "        if ws >= r[\"outtime\"]:\n",
        "            break\n",
        "        rows.append({\n",
        "            \"icustay_id\": r[\"icustay_id\"], \"hadm_id\": r[\"hadm_id\"],\n",
        "            \"subject_id\": r[\"subject_id\"], \"window_id\": w,\n",
        "            \"window_start\": ws, \"window_end\": we,\n",
        "        })\n",
        "windows = pd.DataFrame(rows)\n",
        "print(f\"  ✓ {len(windows):,} windows for {cohort['icustay_id'].nunique():,} stays\")\n",
        "\n",
        "# ============================================================\n",
        "# B — Aggregate features into windows\n",
        "# ============================================================\n",
        "print(\"\\n[B] Aggregating features into windows (this may take a few minutes)...\")\n",
        "records = []\n",
        "for iid, stay_wins in windows.groupby(\"icustay_id\"):\n",
        "    stay_feat = features[features[\"icustay_id\"] == iid]\n",
        "    for _, win in stay_wins.iterrows():\n",
        "        row = {\"icustay_id\": iid, \"window_id\": win[\"window_id\"]}\n",
        "        if not stay_feat.empty:\n",
        "            mask  = (stay_feat[\"charttime\"] >= win[\"window_start\"]) & \\\n",
        "                    (stay_feat[\"charttime\"] <  win[\"window_end\"])\n",
        "            chunk = stay_feat[mask]\n",
        "            if len(chunk):\n",
        "                row.update(chunk.groupby(\"feature\")[\"valuenum\"].mean().to_dict())\n",
        "        records.append(row)\n",
        "\n",
        "states = pd.DataFrame(records)\n",
        "states = (states.sort_values([\"icustay_id\",\"window_id\"])\n",
        "                .groupby(\"icustay_id\", group_keys=False)\n",
        "                .apply(lambda g: g.ffill()))\n",
        "\n",
        "if \"temp_f\" in states.columns and \"temp_c\" not in states.columns:\n",
        "    states[\"temp_c\"] = (states[\"temp_f\"] - 32) * 5 / 9\n",
        "\n",
        "states = states.reset_index(drop=True)\n",
        "states.to_parquet(os.path.join(PROC_DIR, \"states.parquet\"), index=False)\n",
        "print(f\"  ✓ States: {states.shape[0]:,} rows × {states.shape[1]} cols  →  states.parquet\")\n",
        "\n",
        "# ============================================================\n",
        "# C — Discretise actions\n",
        "# ============================================================\n",
        "print(\"\\n[C] Discretising actions...\")\n",
        "\n",
        "def bin_dose(series, n_bins):\n",
        "    bins    = pd.Series(0, index=series.index, dtype=int)\n",
        "    nonzero = series > 0\n",
        "    if nonzero.sum() == 0:\n",
        "        return bins\n",
        "    qs = series[nonzero].quantile([i/(n_bins-1) for i in range(1, n_bins)]).values\n",
        "    for level, threshold in enumerate(qs, start=1):\n",
        "        bins[nonzero & (series <= threshold)] = level\n",
        "    bins[nonzero & (series > qs[-1])] = n_bins - 1\n",
        "    return bins\n",
        "\n",
        "fluids = (raw_actions[raw_actions[\"drug_type\"]==\"iv_fluid\"]\n",
        "          .groupby([\"icustay_id\",\"charttime\"])[\"dose\"].sum().reset_index())\n",
        "vasos  = (raw_actions[raw_actions[\"drug_type\"]==\"vasopressor\"]\n",
        "          .groupby([\"icustay_id\",\"charttime\"])[\"dose\"].sum().reset_index())\n",
        "\n",
        "fluids[\"fluid_bin\"] = bin_dose(fluids[\"dose\"], N_FLUID_BINS)\n",
        "vasos [\"vaso_bin\"]  = bin_dose(vasos [\"dose\"], N_VASO_BINS)\n",
        "\n",
        "actions_binned = fluids.merge(\n",
        "    vasos[[\"icustay_id\",\"charttime\",\"vaso_bin\"]],\n",
        "    on=[\"icustay_id\",\"charttime\"], how=\"outer\"\n",
        ").fillna(0)\n",
        "actions_binned[\"fluid_bin\"] = actions_binned[\"fluid_bin\"].astype(int)\n",
        "actions_binned[\"vaso_bin\"]  = actions_binned[\"vaso_bin\"].astype(int)\n",
        "actions_binned[\"action\"]    = actions_binned[\"fluid_bin\"] * N_VASO_BINS + actions_binned[\"vaso_bin\"]\n",
        "actions_binned.to_parquet(os.path.join(PROC_DIR, \"actions_binned.parquet\"), index=False)\n",
        "print(f\"  ✓ {actions_binned['action'].nunique()} unique actions  →  actions_binned.parquet\")\n",
        "\n",
        "# ============================================================\n",
        "# D — Assign rewards\n",
        "# ============================================================\n",
        "print(\"\\n[D] Assigning rewards...\")\n",
        "final_df = states.merge(mortality, on=\"icustay_id\", how=\"left\")\n",
        "last_win = final_df.groupby(\"icustay_id\")[\"window_id\"].transform(\"max\")\n",
        "is_term  = final_df[\"window_id\"] == last_win\n",
        "final_df[\"reward\"] = 0.0\n",
        "final_df.loc[is_term, \"reward\"] = np.where(\n",
        "    final_df.loc[is_term, \"died_90d\"] == 1,\n",
        "    -TERMINAL_REWARD, +TERMINAL_REWARD\n",
        ")\n",
        "if \"sofa\" in final_df.columns:\n",
        "    sofa_delta = final_df.groupby(\"icustay_id\")[\"sofa\"].diff().fillna(0)\n",
        "    final_df[\"reward\"] -= sofa_delta * SOFA_PENALTY\n",
        "\n",
        "final_df.to_parquet(os.path.join(PROC_DIR, \"sepsis_rl_dataset.parquet\"), index=False)\n",
        "print(f\"  ✓ Final dataset: {final_df.shape[0]:,} rows × {final_df.shape[1]} cols\")\n",
        "print(f\"  ✓ Reward stats:\\n{final_df['reward'].value_counts().sort_index()}\")\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(f\"✓ Processed data saved to: {PROC_DIR}\")\n",
        "print(\"=\"*55)\n",
        "for f in [\"states.parquet\",\"actions_binned.parquet\",\"sepsis_rl_dataset.parquet\"]:\n",
        "    path = os.path.join(PROC_DIR, f)\n",
        "    size_mb = os.path.getsize(path) / 1e6 if os.path.exists(path) else 0\n",
        "    print(f\"  {f:40s}  {size_mb:.1f} MB\")\n",
        "print(\"\\nNext step → run 04_model.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jES-BMCJ0z_Q",
        "outputId": "e9b7e2c3-a103-4aa9-a058-1d9b827e7134"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "STEP 3: Data Processing\n",
            "=======================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "  ✓ Drive mounted\n",
            "  ✓ Output folder ready: /content/drive/MyDrive/sepsis_rl/processed\n",
            "\n",
            "[Loading raw data from Drive]\n",
            "  ✓ cohort.parquet  (88,544 rows)\n",
            "  ✓ features.parquet  (102,961,257 rows)\n",
            "  ✓ actions_raw.parquet  (83,893 rows)\n",
            "  ✓ mortality.parquet  (21,182 rows)\n",
            "\n",
            "[A] Creating 4-hour time windows...\n",
            "  ✓ 1,349,117 windows for 21,182 stays\n",
            "\n",
            "[B] Aggregating features into windows (this may take a few minutes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2400552304.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.ffill()))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ States: 1,349,117 rows × 40 cols  →  states.parquet\n",
            "\n",
            "[C] Discretising actions...\n",
            "  ✓ 1 unique actions  →  actions_binned.parquet\n",
            "\n",
            "[D] Assigning rewards...\n",
            "  ✓ Final dataset: 1,349,117 rows × 43 cols\n",
            "  ✓ Reward stats:\n",
            "reward\n",
            "-15.0      28314\n",
            " 0.0     1260573\n",
            " 15.0      60230\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=======================================================\n",
            "✓ Processed data saved to: /content/drive/MyDrive/sepsis_rl/processed\n",
            "=======================================================\n",
            "  states.parquet                            15.7 MB\n",
            "  actions_binned.parquet                    0.6 MB\n",
            "  sepsis_rl_dataset.parquet                 15.8 MB\n",
            "\n",
            "Next step → run 04_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL**"
      ],
      "metadata": {
        "id": "17AlcNje7hXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 04_model.py  —  Train and evaluate the sepsis RL model\n",
        "#\n",
        "# Reads from Drive  →  sepsis_rl/processed/sepsis_rl_dataset.parquet\n",
        "#\n",
        "# Architecture: Dueling Double DQN (as per WD3QNE paper)\n",
        "#   - State  : normalised clinical feature vector\n",
        "#   - Action : 25 discrete actions (5 fluid bins × 5 vaso bins)\n",
        "#   - Reward : ±15 terminal  +  optional SOFA shaping\n",
        "#\n",
        "# Outputs saved to Drive  →  sepsis_rl/outputs/\n",
        "#   model.pt      (PyTorch state dict)\n",
        "#   metrics.json  (training loss, eval metrics)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import config\n",
        "from auth_drive import setup_drive_folders, load_parquet, save_json, save_file\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL ARCHITECTURE  —  Dueling Double DQN\n",
        "# ============================================================\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Dueling network with separate value and advantage streams.\n",
        "    Inputs  : state_dim  (number of clinical features after imputation)\n",
        "    Outputs : Q-values for each of n_actions discrete actions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dim: int, n_actions: int, hidden: int = 256):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Value stream  V(s)\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(hidden, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "        # Advantage stream  A(s, a)\n",
        "        self.adv_stream = nn.Sequential(\n",
        "            nn.Linear(hidden, 128), nn.ReLU(),\n",
        "            nn.Linear(128, n_actions),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h   = self.shared(x)\n",
        "        V   = self.value_stream(h)\n",
        "        A   = self.adv_stream(h)\n",
        "        # Q(s,a) = V(s) + A(s,a) - mean(A(s,·))\n",
        "        return V + A - A.mean(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# REPLAY BUFFER\n",
        "# ============================================================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int = 50_000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        s, a, r, ns, d = zip(*batch)\n",
        "        return (\n",
        "            torch.FloatTensor(s),\n",
        "            torch.LongTensor(a),\n",
        "            torch.FloatTensor(r),\n",
        "            torch.FloatTensor(ns),\n",
        "            torch.FloatTensor(d),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATA PREPARATION\n",
        "# ============================================================\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    \"heart_rate\", \"sysbp\", \"diasbp\", \"meanbp\", \"resp_rate\",\n",
        "    \"temp_c\", \"spo2\", \"glucose\", \"weight_kg\",\n",
        "    \"creatinine\", \"chloride\", \"bicarbonate\", \"calcium\", \"magnesium\",\n",
        "    \"sodium\", \"potassium\", \"bun\", \"hematocrit\", \"hemoglobin\",\n",
        "    \"platelets\", \"wbc\", \"lactate\", \"ph\", \"pao2\", \"paco2\",\n",
        "    \"base_excess\", \"bilirubin_total\", \"ptt\", \"inr\",\n",
        "    \"alt\", \"ast\", \"alp\", \"crp\",\n",
        "    \"gcs\", \"urine_output\",\n",
        "]\n",
        "\n",
        "\n",
        "def prepare_dataset(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Returns arrays of (states, actions, rewards, next_states, dones)\n",
        "    suitable for experience-replay training.\n",
        "    \"\"\"\n",
        "    # Use only columns present in this cohort extract\n",
        "    feat_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
        "\n",
        "    # Median imputation for remaining NaNs\n",
        "    df[feat_cols] = df[feat_cols].fillna(df[feat_cols].median())\n",
        "\n",
        "    # Normalise features\n",
        "    scaler = StandardScaler()\n",
        "    df[feat_cols] = scaler.fit_transform(df[feat_cols])\n",
        "\n",
        "    # Sort into episodes\n",
        "    df = df.sort_values([\"icustay_id\", \"window_id\"]).reset_index(drop=True)\n",
        "\n",
        "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "\n",
        "    for _, episode in df.groupby(\"icustay_id\"):\n",
        "        ep = episode.reset_index(drop=True)\n",
        "        for t in range(len(ep) - 1):\n",
        "            s  = ep.loc[t,     feat_cols].values.astype(np.float32)\n",
        "            a  = int(ep.loc[t, \"action\"]) if \"action\" in ep.columns else 0\n",
        "            r  = float(ep.loc[t, \"reward\"])\n",
        "            ns = ep.loc[t + 1, feat_cols].values.astype(np.float32)\n",
        "            d  = 1.0 if t == len(ep) - 2 else 0.0\n",
        "            states.append(s);  actions.append(a)\n",
        "            rewards.append(r); next_states.append(ns)\n",
        "            dones.append(d)\n",
        "\n",
        "    return (\n",
        "        np.array(states),\n",
        "        np.array(actions),\n",
        "        np.array(rewards),\n",
        "        np.array(next_states),\n",
        "        np.array(dones),\n",
        "        feat_cols,\n",
        "        scaler,\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "def train(\n",
        "    states, actions, rewards, next_states, dones,\n",
        "    state_dim: int,\n",
        "    n_actions:  int = 25,\n",
        "    n_epochs:   int = 200,\n",
        "    batch_size: int = 512,\n",
        "    lr:         float = 1e-4,\n",
        "    gamma:      float = 0.99,\n",
        "    tau:        float = 0.01,\n",
        "    device:     str = \"cpu\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Offline Dueling Double DQN training from a fixed replay buffer.\n",
        "    Returns (online_net, metrics_dict).\n",
        "    \"\"\"\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"  Training on: {device}  |  state_dim={state_dim}  |  n_actions={n_actions}\")\n",
        "\n",
        "    online_net = DuelingDQN(state_dim, n_actions).to(device)\n",
        "    target_net = DuelingDQN(state_dim, n_actions).to(device)\n",
        "    target_net.load_state_dict(online_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(online_net.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    loss_fn   = nn.SmoothL1Loss()\n",
        "\n",
        "    # Populate replay buffer\n",
        "    buffer = ReplayBuffer(capacity=len(states))\n",
        "    for i in range(len(states)):\n",
        "        buffer.push(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        if len(buffer) < batch_size:\n",
        "            break\n",
        "\n",
        "        s, a, r, ns, d = buffer.sample(batch_size)\n",
        "        s, a, r, ns, d = s.to(device), a.to(device), r.to(device), ns.to(device), d.to(device)\n",
        "\n",
        "        # Double DQN target\n",
        "        with torch.no_grad():\n",
        "            best_actions = online_net(ns).argmax(dim=1)\n",
        "            target_q     = target_net(ns).gather(1, best_actions.unsqueeze(1)).squeeze()\n",
        "            y            = r + gamma * target_q * (1 - d)\n",
        "\n",
        "        q_vals = online_net(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "        loss   = loss_fn(q_vals, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(online_net.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Soft update target network\n",
        "        for p_online, p_target in zip(online_net.parameters(), target_net.parameters()):\n",
        "            p_target.data.copy_(tau * p_online.data + (1 - tau) * p_target.data)\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        loss_history.append(loss_val)\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"  Epoch {epoch:>4d}/{n_epochs}  —  loss: {loss_val:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        \"n_transitions\":    int(len(states)),\n",
        "        \"state_dim\":        state_dim,\n",
        "        \"n_actions\":        n_actions,\n",
        "        \"final_loss\":       float(loss_history[-1]) if loss_history else None,\n",
        "        \"mean_loss\":        float(np.mean(loss_history)) if loss_history else None,\n",
        "        \"loss_history\":     loss_history,\n",
        "    }\n",
        "\n",
        "    return online_net.cpu(), metrics\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 55)\n",
        "    print(\"STEP 4: Model Training (Dueling Double DQN)\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    folders  = setup_drive_folders()\n",
        "    proc_path = folders[\"processed\"]\n",
        "    out_path  = folders[\"outputs\"]\n",
        "\n",
        "    # --- Load processed dataset ---\n",
        "    print(\"\\n[Loading processed dataset from Drive]\")\n",
        "    df = load_parquet(proc_path, config.PROC_DATASET_FILE)\n",
        "    print(f\"  Loaded {len(df):,} rows × {df.shape[1]} columns\")\n",
        "\n",
        "    # --- Prepare ---\n",
        "    print(\"\\n[Preparing training data]\")\n",
        "    states, actions, rewards, next_states, dones, feat_cols, scaler = \\\n",
        "        prepare_dataset(df)\n",
        "    print(f\"  Transitions: {len(states):,}  |  Features used: {len(feat_cols)}\")\n",
        "\n",
        "    # --- Train ---\n",
        "    print(\"\\n[Training]\")\n",
        "    model, metrics = train(\n",
        "        states, actions, rewards, next_states, dones,\n",
        "        state_dim = len(feat_cols),\n",
        "    )\n",
        "    metrics[\"feature_columns\"] = feat_cols\n",
        "\n",
        "    # --- Save model directly to Drive ---\n",
        "    print(\"\\n[Saving model and metrics to Drive]\")\n",
        "\n",
        "    import tempfile, os\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".pt\", delete=False) as tmp:\n",
        "        model_path = tmp.name\n",
        "    torch.save(\n",
        "        {\"model_state_dict\": model.state_dict(), \"feature_columns\": feat_cols},\n",
        "        model_path,\n",
        "    )\n",
        "    save_file(model_path, out_path)\n",
        "    os.unlink(model_path)\n",
        "\n",
        "    save_json(metrics, out_path, config.OUTPUT_METRICS_FILE)\n",
        "\n",
        "    print(f\"\\nFinal loss : {metrics['final_loss']:.4f}\")\n",
        "    print(f\"Mean loss  : {metrics['mean_loss']:.4f}\")\n",
        "    print(\"\\n✓ Model saved to Drive → sepsis_rl/outputs/\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "y5QtWMDk7ade"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE OUTPUT**"
      ],
      "metadata": {
        "id": "pBwo4ycI7y_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 05_save_outputs.py  —  Generate evaluation plots & summary report,\n",
        "#                         then upload everything to Drive outputs/\n",
        "#\n",
        "# Reads from Drive  →  sepsis_rl/outputs/model.pt, metrics.json\n",
        "#                   →  sepsis_rl/processed/sepsis_rl_dataset.parquet\n",
        "#\n",
        "# Writes to Drive   →  sepsis_rl/outputs/\n",
        "#   loss_curve.png\n",
        "#   action_distribution.png\n",
        "#   reward_distribution.png\n",
        "#   summary_report.json\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")   # non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import config\n",
        "from auth_drive import setup_drive_folders, load_parquet, load_json, save_file\n",
        "from model import DuelingDQN, FEATURE_COLS\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LOAD HELPERS\n",
        "# ============================================================\n",
        "\n",
        "def load_model_from_drive(folder_path: str, state_dim: int, n_actions: int):\n",
        "    import os\n",
        "    path = os.path.join(folder_path, config.OUTPUT_MODEL_FILE)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"'{config.OUTPUT_MODEL_FILE}' not found at {path}. \"\n",
        "            \"Run 04_model.py first.\"\n",
        "        )\n",
        "    checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "    model = DuelingDQN(state_dim, n_actions)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "    print(f\"  Loaded model ← {path}\")\n",
        "    return model, checkpoint.get(\"feature_columns\", [])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PLOT FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def plot_loss_curve(loss_history: list, save_path: str):\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.plot(loss_history, color=\"#2563EB\", linewidth=1.2, alpha=0.9)\n",
        "    ax.set_xlabel(\"Training Epoch\")\n",
        "    ax.set_ylabel(\"Smooth L1 Loss\")\n",
        "    ax.set_title(\"Dueling Double DQN — Training Loss Curve\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  Saved loss curve → {save_path}\")\n",
        "\n",
        "\n",
        "def plot_action_distribution(df: pd.DataFrame, save_path: str):\n",
        "    if \"action\" not in df.columns:\n",
        "        print(\"  No 'action' column — skipping action distribution plot.\")\n",
        "        return\n",
        "\n",
        "    n_actions = config.N_FLUID_BINS * config.N_VASO_BINS\n",
        "    counts    = df[\"action\"].value_counts().reindex(range(n_actions), fill_value=0)\n",
        "\n",
        "    fluid_labels = [f\"Fluid {i}\" for i in range(config.N_FLUID_BINS)]\n",
        "    vaso_labels  = [f\"Vaso {j}\"  for j in range(config.N_VASO_BINS)]\n",
        "\n",
        "    matrix = counts.values.reshape(config.N_FLUID_BINS, config.N_VASO_BINS)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    im = ax.imshow(matrix, cmap=\"YlOrRd\")\n",
        "    ax.set_xticks(range(config.N_VASO_BINS));  ax.set_xticklabels(vaso_labels)\n",
        "    ax.set_yticks(range(config.N_FLUID_BINS)); ax.set_yticklabels(fluid_labels)\n",
        "    ax.set_title(\"Action Frequency Heatmap\\n(Fluid bin × Vasopressor bin)\")\n",
        "    plt.colorbar(im, ax=ax, label=\"Count\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  Saved action distribution → {save_path}\")\n",
        "\n",
        "\n",
        "def plot_reward_distribution(df: pd.DataFrame, save_path: str):\n",
        "    if \"reward\" not in df.columns:\n",
        "        print(\"  No 'reward' column — skipping reward distribution plot.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # All rewards\n",
        "    axes[0].hist(df[\"reward\"], bins=30, color=\"#10B981\", edgecolor=\"white\")\n",
        "    axes[0].set_title(\"All-Step Reward Distribution\")\n",
        "    axes[0].set_xlabel(\"Reward\"); axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "    # Terminal rewards only\n",
        "    last_w    = df.groupby(\"icustay_id\")[\"window_id\"].transform(\"max\")\n",
        "    term_df   = df[df[\"window_id\"] == last_w]\n",
        "    term_vals = term_df[\"reward\"].value_counts().sort_index()\n",
        "    axes[1].bar(\n",
        "        term_vals.index.astype(str), term_vals.values,\n",
        "        color=[\"#EF4444\" if v < 0 else \"#10B981\" for v in term_vals.index]\n",
        "    )\n",
        "    axes[1].set_title(\"Terminal Reward Distribution\")\n",
        "    axes[1].set_xlabel(\"Reward\"); axes[1].set_ylabel(\"Stays\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  Saved reward distribution → {save_path}\")\n",
        "\n",
        "\n",
        "def plot_q_value_sample(\n",
        "    model: DuelingDQN,\n",
        "    df: pd.DataFrame,\n",
        "    feat_cols: list,\n",
        "    save_path: str,\n",
        "    n_sample: int = 500,\n",
        "):\n",
        "    \"\"\"Plot distribution of max Q-values over a random sample of states.\"\"\"\n",
        "    df2 = df[feat_cols].copy().fillna(df[feat_cols].median())\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df2)\n",
        "    idx    = np.random.choice(len(scaled), min(n_sample, len(scaled)), replace=False)\n",
        "    sample = torch.FloatTensor(scaled[idx])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        q_vals = model(sample).numpy()\n",
        "\n",
        "    max_q = q_vals.max(axis=1)\n",
        "    best_a = q_vals.argmax(axis=1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    axes[0].hist(max_q, bins=30, color=\"#6366F1\", edgecolor=\"white\")\n",
        "    axes[0].set_title(\"Max Q-Value Distribution (sample)\")\n",
        "    axes[0].set_xlabel(\"Max Q(s,a)\")\n",
        "\n",
        "    n_actions = config.N_FLUID_BINS * config.N_VASO_BINS\n",
        "    counts    = np.bincount(best_a, minlength=n_actions)\n",
        "    axes[1].bar(range(n_actions), counts, color=\"#6366F1\")\n",
        "    axes[1].set_title(\"Greedy Action Distribution (sample)\")\n",
        "    axes[1].set_xlabel(\"Action index\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"  Saved Q-value sample plot → {save_path}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY REPORT\n",
        "# ============================================================\n",
        "\n",
        "def build_summary(metrics: dict, df: pd.DataFrame) -> dict:\n",
        "    n_stays = df[\"icustay_id\"].nunique() if \"icustay_id\" in df.columns else None\n",
        "\n",
        "    last_w = df.groupby(\"icustay_id\")[\"window_id\"].transform(\"max\")\n",
        "    term_r = df.loc[df[\"window_id\"] == last_w, \"reward\"]\n",
        "\n",
        "    return {\n",
        "        \"n_icu_stays\":           n_stays,\n",
        "        \"n_transitions\":         metrics.get(\"n_transitions\"),\n",
        "        \"state_dim\":             metrics.get(\"state_dim\"),\n",
        "        \"n_actions\":             metrics.get(\"n_actions\"),\n",
        "        \"training_final_loss\":   metrics.get(\"final_loss\"),\n",
        "        \"training_mean_loss\":    metrics.get(\"mean_loss\"),\n",
        "        \"pct_positive_terminal\": float((term_r > 0).mean()),\n",
        "        \"pct_negative_terminal\": float((term_r < 0).mean()),\n",
        "        \"feature_columns\":       metrics.get(\"feature_columns\", []),\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 55)\n",
        "    print(\"STEP 5: Save Outputs to Drive\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    folders   = setup_drive_folders()\n",
        "    proc_path = folders[\"processed\"]\n",
        "    out_path  = folders[\"outputs\"]\n",
        "\n",
        "    # --- Load artefacts directly from Drive filesystem ---\n",
        "    print(\"\\n[Loading artefacts from Drive]\")\n",
        "    metrics = load_json(out_path,  config.OUTPUT_METRICS_FILE)\n",
        "    df      = load_parquet(proc_path, config.PROC_DATASET_FILE)\n",
        "\n",
        "    feat_cols = metrics.get(\"feature_columns\") or \\\n",
        "                [c for c in FEATURE_COLS if c in df.columns]\n",
        "    state_dim = len(feat_cols)\n",
        "    n_actions = config.N_FLUID_BINS * config.N_VASO_BINS\n",
        "\n",
        "    model, _ = load_model_from_drive(out_path, state_dim, n_actions)\n",
        "\n",
        "    # --- Generate plots directly into Drive outputs folder ---\n",
        "    print(\"\\n[Generating plots]\")\n",
        "    import os\n",
        "    loss_path   = os.path.join(out_path, \"loss_curve.png\")\n",
        "    action_path = os.path.join(out_path, \"action_distribution.png\")\n",
        "    reward_path = os.path.join(out_path, \"reward_distribution.png\")\n",
        "    qval_path   = os.path.join(out_path, \"q_value_sample.png\")\n",
        "\n",
        "    plot_loss_curve(metrics.get(\"loss_history\", []), loss_path)\n",
        "    plot_action_distribution(df, action_path)\n",
        "    plot_reward_distribution(df, reward_path)\n",
        "    plot_q_value_sample(model, df, feat_cols, qval_path)\n",
        "\n",
        "    # --- Build and save summary report ---\n",
        "    summary = build_summary(metrics, df)\n",
        "    save_json(summary, out_path, \"summary_report.json\")\n",
        "    print(f\"  Summary: {summary}\")\n",
        "\n",
        "    print(\"\\n✓ All outputs saved to Drive → sepsis_rl/outputs/\")\n",
        "    print(\"  Files written:\")\n",
        "    print(\"    loss_curve.png\")\n",
        "    print(\"    action_distribution.png\")\n",
        "    print(\"    reward_distribution.png\")\n",
        "    print(\"    q_value_sample.png\")\n",
        "    print(\"    summary_report.json\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "khTtZ8qO8Tq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}